{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9063f92f",
   "metadata": {},
   "source": [
    "# Wharton High School Data Science Competition\n",
    "## ELO Rating Model for Basketball Team Rankings\n",
    "\n",
    "This notebook demonstrates how to build and use an ELO rating system to rank basketball teams and predict game outcomes, following the approach outlined in the Wharton competition playbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df3cbc",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from elo_model import ELOModel, AdaptiveELOModel\n",
    "from data_prep import DataPreprocessor, PossessionAdjustmentCalculator\n",
    "from ensemble import EnsemblePredictor, create_default_ensemble, PointDifferentialModel, WinLossModel\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae5ae1",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Competition Data\n",
    "\n",
    "We'll create sample basketball game data to demonstrate the ELO model in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample basketball game data\n",
    "np.random.seed(42)\n",
    "\n",
    "teams = ['Duke', 'North Carolina', 'Kansas', 'UCLA', 'Indiana', 'Arizona', \n",
    "         'Villanova', 'Gonzaga', 'Texas', 'Michigan', 'Notre Dame', 'Wisconsin']\n",
    "\n",
    "# Generate sample games\n",
    "games_data = []\n",
    "start_date = datetime(2024, 11, 1)\n",
    "\n",
    "for i in range(100):\n",
    "    team_a = np.random.choice(teams)\n",
    "    team_b = np.random.choice([t for t in teams if t != team_a])\n",
    "    \n",
    "    # Simulate scores (random but reasonable for basketball)\n",
    "    score_a = np.random.normal(75, 8)\n",
    "    score_b = np.random.normal(72, 8)\n",
    "    \n",
    "    games_data.append({\n",
    "        'date': start_date + timedelta(days=i),\n",
    "        'team_a': team_a,\n",
    "        'team_b': team_b,\n",
    "        'score_a': int(max(50, score_a)),\n",
    "        'score_b': int(max(50, score_b)),\n",
    "        'location': 'Home' if np.random.rand() > 0.5 else 'Away'\n",
    "    })\n",
    "\n",
    "games_df = pd.DataFrame(games_data)\n",
    "games_df = games_df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset: {len(games_df)} games from {games_df['date'].min().date()} to {games_df['date'].max().date()}\")\n",
    "print(f\"\\nUnique teams: {len(set(games_df['team_a'].unique()) | set(games_df['team_b'].unique()))}\")\n",
    "print(\"\\nFirst 5 games:\")\n",
    "print(games_df.head())\n",
    "print(\"\\nData summary:\")\n",
    "print(games_df[['score_a', 'score_b']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebfa22",
   "metadata": {},
   "source": [
    "## 3. Understand ELO Rating Fundamentals\n",
    "\n",
    "The ELO rating system is a method for calculating the relative skill levels of teams based on game outcomes.\n",
    "\n",
    "### ELO Formula\n",
    "\n",
    "The new rating is calculated as:\n",
    "\n",
    "$$R_{new} = R_{old} + K \\times (S - E)$$\n",
    "\n",
    "Where:\n",
    "- $R_{old}$ = Team's current ELO rating\n",
    "- $R_{new}$ = Team's updated ELO rating  \n",
    "- $K$ = K-factor (typically 32, controls how much ratings change per game)\n",
    "- $S$ = Actual result (1.0 for win, 0.5 for tie, 0.0 for loss)\n",
    "- $E$ = Expected win probability\n",
    "\n",
    "The expected win probability is:\n",
    "\n",
    "$$E = \\frac{1}{1 + 10^{(R_{opponent} - R_{team})/400}}$$\n",
    "\n",
    "### Key Insights from Wharton Playbook:\n",
    "\n",
    "1. **Simple yet powerful**: ELO captures team strength accounting for opponent quality\n",
    "2. **Adaptive**: Ratings adjust based on actual outcomes\n",
    "3. **Contextual**: A win over a strong team increases rating more than a win over a weak team\n",
    "4. **Real-time**: Can be updated after each game for current rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec3420",
   "metadata": {},
   "source": [
    "## 4. Initialize Player/Team Ratings\n",
    "\n",
    "Create initial ELO ratings for all teams. Teams typically start at 1500, but can be adjusted based on preseason expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ELO model\n",
    "elo_model = ELOModel(initial_rating=1500, k_factor=32)\n",
    "\n",
    "# Initialize all teams\n",
    "all_teams = list(set(games_df['team_a'].unique()) | set(games_df['team_b'].unique()))\n",
    "elo_model.initialize_teams(all_teams)\n",
    "\n",
    "# Display initial ratings\n",
    "initial_ratings = pd.DataFrame([\n",
    "    {'team': team, 'rating': elo_model.get_rating(team)}\n",
    "    for team in all_teams\n",
    "]).sort_values('rating', ascending=False)\n",
    "\n",
    "print(\"Initial ELO Ratings:\")\n",
    "print(initial_ratings)\n",
    "print(f\"\\nTotal teams: {len(all_teams)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d16b7",
   "metadata": {},
   "source": [
    "## 5. Implement ELO Rating Calculation\n",
    "\n",
    "The ELO model is already implemented in the `ELOModel` class with key methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculate expected win probability\n",
    "team1 = 'Duke'\n",
    "team2 = 'Kansas'\n",
    "\n",
    "rating1 = elo_model.get_rating(team1)\n",
    "rating2 = elo_model.get_rating(team2)\n",
    "\n",
    "expected_prob = elo_model.expected_score(rating1, rating2)\n",
    "\n",
    "print(f\"{team1} (Rating: {rating1:.1f}) vs {team2} (Rating: {rating2:.1f})\")\n",
    "print(f\"Expected win probability for {team1}: {expected_prob:.2%}\")\n",
    "print(f\"Expected win probability for {team2}: {(1-expected_prob):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4120b",
   "metadata": {},
   "source": [
    "## 6. Update Ratings After Matches\n",
    "\n",
    "Process all games in chronological order, updating team ratings after each game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9386a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all games and update ELO ratings\n",
    "print(\"Processing games and updating ratings...\")\n",
    "\n",
    "for idx, row in games_df.iterrows():\n",
    "    team_a = row['team_a']\n",
    "    team_b = row['team_b']\n",
    "    score_a = row['score_a']\n",
    "    score_b = row['score_b']\n",
    "    \n",
    "    elo_model.process_game(team_a, team_b, score_a, score_b)\n",
    "    \n",
    "    if (idx + 1) % 25 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(games_df)} games\")\n",
    "\n",
    "print(f\"\\nCompleted processing all {len(games_df)} games!\")\n",
    "print(f\"Total rating updates: {len(elo_model.history)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9982019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final rankings\n",
    "final_rankings = elo_model.get_rankings()\n",
    "print(\"\\nFinal ELO Rankings:\")\n",
    "print(final_rankings.to_string(index=False))\n",
    "\n",
    "# Show some rating update examples\n",
    "print(\"\\n\\nSample Rating Updates (first 10):\")\n",
    "history_df = elo_model.get_history_dataframe()\n",
    "print(history_df.head(10)[['team', 'opponent', 'result', 'old_rating', 'new_rating', 'rating_change']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096920b6",
   "metadata": {},
   "source": [
    "## 7. Visualize Rating Changes Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fb72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track ratings over time for top teams\n",
    "history_df = elo_model.get_history_dataframe()\n",
    "\n",
    "# Get top 5 final teams\n",
    "top_5_teams = final_rankings.head(5)['team'].tolist()\n",
    "\n",
    "# Create a plot showing rating progression\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "for team in top_5_teams:\n",
    "    team_history = history_df[history_df['team'] == team].reset_index(drop=True)\n",
    "    team_history['game_num'] = range(len(team_history))\n",
    "    ax.plot(team_history['game_num'], team_history['new_rating'], \n",
    "            marker='o', label=team, linewidth=2, markersize=4, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Game Number', fontsize=12)\n",
    "ax.set_ylabel('ELO Rating', fontsize=12)\n",
    "ax.set_title('ELO Rating Progression for Top 5 Teams', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart shows how top-rated teams' ELO ratings changed throughout the season.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b98b6",
   "metadata": {},
   "source": [
    "## 8. Analyze Rating Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of final ratings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ratings = final_rankings['rating'].values\n",
    "axes[0].hist(ratings, bins=15, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('ELO Rating', fontsize=11)\n",
    "axes[0].set_ylabel('Number of Teams', fontsize=11)\n",
    "axes[0].set_title('Distribution of Final ELO Ratings', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(ratings, vert=True)\n",
    "axes[1].set_ylabel('ELO Rating', fontsize=11)\n",
    "axes[1].set_title('ELO Rating Box Plot', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"Final Rating Distribution Statistics:\")\n",
    "print(f\"Mean: {ratings.mean():.1f}\")\n",
    "print(f\"Median: {np.median(ratings):.1f}\")\n",
    "print(f\"Std Dev: {ratings.std():.1f}\")\n",
    "print(f\"Min: {ratings.min():.1f}\")\n",
    "print(f\"Max: {ratings.max():.1f}\")\n",
    "print(f\"Range: {ratings.max() - ratings.min():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b25bd",
   "metadata": {},
   "source": [
    "## 9. Export Results and Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc13eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final rankings to CSV\n",
    "output_dir = '../data'\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "final_rankings.to_csv(f'{output_dir}/final_elo_rankings.csv', index=False)\n",
    "history_df.to_csv(f'{output_dir}/rating_history.csv', index=False)\n",
    "\n",
    "print(\"✓ Exported final_elo_rankings.csv\")\n",
    "print(\"✓ Exported rating_history.csv\")\n",
    "\n",
    "print(\"\\nFinal Rankings Summary:\")\n",
    "print(final_rankings.to_string(index=False))\n",
    "\n",
    "# Show statistics by rank quartile\n",
    "print(\"\\n\\nRatings by Quartile:\")\n",
    "quantiles = [0.25, 0.5, 0.75]\n",
    "for q in quantiles:\n",
    "    val = final_rankings['rating'].quantile(q)\n",
    "    print(f\"{q:.0%} quartile: {val:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
